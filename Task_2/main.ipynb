{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers bitsandbytes accelerate -qqq\n",
    "# %pip install swifter pillow pytorchvideo einops peft hnswlib datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlavaForConditionalGeneration,BitsAndBytesConfig, AutoProcessor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.models.llava.modeling_llava import LlavaCausalLMOutputWithPast\n",
    "from transformers.cache_utils import Cache\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import swifter\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imageio\n",
    "from urllib.request import urlopen\n",
    "from Encoder import MultiModalEncoder\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    model_name = \"llava-hf/llava-1.5-7b-hf\" \n",
    "    quantisation_4_bit = False\n",
    "    quantisation_8_bit = True\n",
    "    \n",
    "    batch_size = 1\n",
    "    grad_acc_steps = 8\n",
    "    device = 'cuda'\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.model_name,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "        args.model_name,\n",
    "        # quantization_config=bnb_config if args.quantisation_4_bit else None,# 4-bit quantisation\n",
    "        load_in_8bit = True if args.quantisation_8_bit else None, # 8-bit quantisation\n",
    "        device_map={\"\": 0}, # Single GPU\n",
    "        trust_remote_code=True\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor =AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve closest tweets by media embedding, likes and same company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analogy retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from companies import companies_arr\n",
    "def get_img_url(string):\n",
    "    url = re.findall(f\"Url='(.+?)'\",string)\n",
    "    if url:return url[0]\n",
    "    return\n",
    "\n",
    "\n",
    "def get_vid_url(string):\n",
    "    url = re.findall(r\"VideoVariant\\(contentType='video/mp4', url='(.+?)',\",string)\n",
    "    if url:return url[0]\n",
    "    return\n",
    "\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "def get_image_from_url(url, timeout=5):\n",
    "    # Create a session object if it's not given\n",
    "    with requests.Session() as session:\n",
    "        try:\n",
    "            with session.get(url, stream=True, timeout=timeout) as response:\n",
    "                response.raise_for_status()  # Raises HTTPError for bad HTTP status codes\n",
    "                if 'image' in response.headers.get('Content-Type', '').lower():\n",
    "                    # Use BytesIO to load the image from the response content\n",
    "                    image = Image.open(BytesIO(response.content))\n",
    "                    \n",
    "                    # Convert the image to RGB if it's not already in RGB mode\n",
    "                    if image.mode not in ('RGB'):\n",
    "                        image = image.convert('RGB')\n",
    "                    \n",
    "                    return image\n",
    "                else:\n",
    "                    return None\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "def get_video_from_url(url):\n",
    "    try:\n",
    "        with urlopen(url) as response:\n",
    "            video_data = response.read()\n",
    "            video_bytes = BytesIO(video_data)\n",
    "            return video_bytes\n",
    "    except:\n",
    "        return\n",
    "\n",
    "\n",
    "def filter_date_top_k_likes(df, likes, input_date, inp_unseen_brand, k=5):\n",
    "    input_date = datetime.strptime(input_date, '%Y-%m-%d %H:%M:%S')\n",
    "    six_months = timedelta(days=30*6)\n",
    "    start_date = input_date - six_months\n",
    "    end_date = input_date\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    filtered_df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n",
    "    \n",
    "    if filtered_df.empty:\n",
    "        print(f\"No matching rows found for {input_date}\")\n",
    "        return None\n",
    "    \n",
    "    companies=[]\n",
    "    for row in companies_arr:\n",
    "        if inp_unseen_brand in row:\n",
    "            companies=row\n",
    "    \n",
    "    filtered2_df = filtered_df[filtered_df['inferred company'].isin(companies)]\n",
    "    \n",
    "    filtered2_df['likes_distance'] = abs(filtered2_df['likes'] - likes)\n",
    "    top_k_df = filtered2_df.nsmallest(k, 'likes_distance')\n",
    "    \n",
    "    return top_k_df.drop(columns=['likes_distance']).iloc[:k]\n",
    "\n",
    "\n",
    "\n",
    "analogies = pd.read_csv('dump/analogies.csv')\n",
    "training = pd.read_csv('dump/training.csv')\n",
    "\n",
    "\n",
    "\n",
    "likes = 110\n",
    "input_date = '2019-03-13 12:32:12'\n",
    "inp_unseen_brand = 'sony'\n",
    "\n",
    "retrieved = filter_date_top_k_likes(training, likes, input_date, inp_unseen_brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = MultiModalEncoder('cuda')\n",
    "\n",
    "\"\"\"\n",
    "Example:\n",
    "vid = get_video_from_url(r'https://video.twimg.com/amplify_video/106741146272395776/vid/240x240/hXX2ch5Jrk44xYW0.mp4?tag=8')\n",
    "img = get_image_from_url(r'https://pbs.twimg.com/media/Eo8N3JLVoAAlDJT?format=jpg&name=small')\n",
    "\n",
    "output = encoder({'image':img,'video':vid})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docarray.index import HnswDocumentIndex\n",
    "from docarray import BaseDoc, DocList,DocVec\n",
    "from docarray.typing import ImageUrl, VideoUrl, ID, NdArray\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class Doc(BaseDoc):\n",
    "    id: ID\n",
    "    date: Optional[str] = None\n",
    "    likes: Optional[int] = None\n",
    "    content: Optional[str] = None\n",
    "    username: Optional[str] = None\n",
    "    media: Optional[str] = None\n",
    "    inferred_company: Optional[str] = None\n",
    "    img_url: Optional[ImageUrl] = None\n",
    "    vid_url: Optional[VideoUrl] = None\n",
    "    img_vector: NdArray[768]\n",
    "    vid_vector: NdArray[768]\n",
    "    text_vector: NdArray[768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = DocList[Doc].load_binary('dump/analogies_embeddings.pickle', compress=None, protocol='pickle')\n",
    "doc_index = HnswDocumentIndex[Doc](work_dir='./tmp')\n",
    "doc_index.index(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_list(lst, func):\n",
    "    none_positions = [i for i, x in enumerate(lst) if x is None]\n",
    "    processed_list = [x for x in lst if x is not None]\n",
    "    processed_list = func(processed_list)\n",
    "    for pos in none_positions:\n",
    "        processed_list.insert(pos, None)\n",
    "    return processed_list\n",
    "\n",
    "import torch\n",
    "@torch.inference_mode()\n",
    "def _get_embedding(to_encode,key):\n",
    "    encoded = list(encoder({key:to_encode})[key].pooler_output)\n",
    "    encoded = [x.cpu().numpy() for x in encoded]\n",
    "    return encoded\n",
    "\n",
    "def get_embedding(to_encode,key):\n",
    "    if not isinstance(to_encode, list): to_encode = [to_encode]\n",
    "    return process_list(to_encode, lambda x: _get_embedding(x,key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = training.iloc[0]\n",
    "\n",
    "def retrieve(row):\n",
    "    retrieved = {}\n",
    "    try:\n",
    "        if row.img_url is not None:\n",
    "            url = row.img_url\n",
    "            img = get_image_from_url(url)\n",
    "            embed = get_embedding([img],'image')[0]\n",
    "            results = doc_index.find(query=embed,search_field='img_vector',limit=3)\n",
    "        if not pd.isna(row.vid_url):\n",
    "            url = row.vid_url\n",
    "            vid = get_video_from_url(url)\n",
    "            embed = get_embedding(vid,'video')[0]\n",
    "            results = doc_index.find(query=embed,search_field='vid_vector',limit=3)\n",
    "        retrieved['media'] = [{'content':doc.content, 'user':doc.username, 'likes':doc.likes, 'company':doc.inferred_company} for doc in results.documents]\n",
    "    except:\n",
    "        retrieved['media'] = []\n",
    "    rtr = filter_date_top_k_likes(analogies, row.likes, str(row['date']), row['inferred company'],5)\n",
    "    retrieved['likes_company'] = [{'content':doc.content, 'user':doc.username, 'likes':doc.likes, 'company':doc['inferred company']} for i,doc in rtr.iterrows()]\n",
    "    return retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtr = retrieve(training.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\\\n",
    "\n",
    "\n",
    "    <s>[INST] \n",
    "Using the given tweets as reference construct a tweet which has analogical similarity to those, conditioned on the fact that the post is extremely viral.\n",
    "\n",
    "Reference tweets with similar likes: \n",
    "\n",
    "Tweet: ```What a great day to BE part of the #BTSARMY. Shop the deluxe version of <mention>'s new album: <hyperlink> <hyperlink>```\n",
    "Username: Target\n",
    "Company: Target\n",
    "\n",
    "Tweet: ```Your grand ideas will never go off track. ðŸ’¡ #GalaxyNote20 #GalaxyxBTS <mention> ðŸ‘” Learn more: <hyperlink> <hyperlink>```\n",
    "Username: Samsung\n",
    "Company: Samsung\n",
    "\n",
    "\n",
    "The tweet is written by the user spotify belong to spotify.\n",
    "\n",
    "New Tweet:\n",
    "\n",
    "Given the following analogies:\n",
    "\n",
    "by similar likes and companies: \n",
    "{}\n",
    "\n",
    "by media similarity:\n",
    "{}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_likes(likes):\n",
    "        if likes > 10000:\n",
    "            return \"Viral\"\n",
    "        elif likes > 1000:\n",
    "            return \"High Engagement\"\n",
    "        elif likes > 100:\n",
    "            return \"Moderate Engagement\"\n",
    "        else:\n",
    "            return \"Low Engagement\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(row):\n",
    "    rtr = retrieve(row)\n",
    "    template = '''\\\n",
    "Using the given tweets as reference construct a tweet which has analogical similarity to those.\\\n",
    "\n",
    "Reference tweets with similar likes and companies:\n",
    "{likes_based}\n",
    "\n",
    "Reference tweets with similar media:\n",
    "{media_based}\n",
    "\n",
    "\n",
    "Generated:\n",
    "User: {username}\n",
    "Likes: {likes}\n",
    "Level: {likes_level}\n",
    "Tweet: {tweet}\n",
    "'''\n",
    "    return template.format(\n",
    "        likes_based = rtr['likes_company'],\n",
    "        media_based = rtr['media'],\n",
    "        tweet = row.content,\n",
    "        username = row.username,\n",
    "        likes = row.likes,\n",
    "        likes_level = classify_likes(row.likes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "prompts = joblib.Parallel(n_jobs=-1,backend='threading')(joblib.delayed(make_prompt)(row) for i,row in training.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "train_df = pd.DataFrame({'prompts':prompts[:10]})\n",
    "test_df = pd.DataFrame({'prompts':prompts[10:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "train_ds = HFDataset.from_pandas(train_df)\n",
    "test_ds = HFDataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results_latest\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim='paged_adamw_32bit',\n",
    "    # save_steps=250,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "    save_steps=25, \n",
    "    learning_rate=2e-4,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to = 'wandb'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset = test_ds,\n",
    "    peft_config=config,\n",
    "    dataset_text_field=\"prompts\",\n",
    "    max_seq_length=1024, # Adjust accordingly\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=True,\n",
    ")\n",
    "\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"output_dir\") # saves lora again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extention to Video and Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.models.llava.modeling_llava import LlavaCausalLMOutputWithPast\n",
    "from transformers.cache_utils import Cache\n",
    "\n",
    "class MultiMediaConditionalGeneration(LlavaForConditionalGeneration):\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        pixel_values: torch.FloatTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        vision_feature_layer: Optional[int] = None,\n",
    "        vision_feature_select_strategy: Optional[str] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, LlavaCausalLMOutputWithPast]:\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            # 1. Extra the input embeddings\n",
    "            inputs_embeds = self.get_input_embeddings()(input_ids)\n",
    "\n",
    "            # 2. Merge text and images\n",
    "            if pixel_values is not None and input_ids.shape[1] != 1:\n",
    "                inputs_embeds, attention_mask, position_ids = self._merge_input_ids_with_image_features(\n",
    "                    pixel_values, inputs_embeds, input_ids, attention_mask, position_ids\n",
    "                )\n",
    "                if labels is None:\n",
    "                    labels = torch.full_like(attention_mask, self.config.ignore_index).to(torch.long)\n",
    "            else:\n",
    "                # In case input_ids.shape[1] == 1 & pixel_values==None & past_key_values != None, we are in the case of\n",
    "                # generation with cache\n",
    "                if past_key_values is not None and pixel_values is not None and input_ids.shape[1] == 1:\n",
    "                    # Retrieve the first layer to inspect the logits and mask out the hidden states\n",
    "                    # that are set to 0\n",
    "                    first_layer_past_key_value = past_key_values[0][0][:, 0, :, 0]\n",
    "                    batch_index, non_attended_tokens = torch.where(first_layer_past_key_value == 0)\n",
    "                    # Get the target length\n",
    "                    target_seqlen = first_layer_past_key_value.shape[-1] + 1\n",
    "\n",
    "                    extended_attention_mask = torch.ones(\n",
    "                        (attention_mask.shape[0], target_seqlen - attention_mask.shape[1]),\n",
    "                        dtype=attention_mask.dtype,\n",
    "                        device=attention_mask.device,\n",
    "                    )\n",
    "\n",
    "                    # Zero-out the places where we don't need to attend\n",
    "                    extended_attention_mask[batch_index, non_attended_tokens] = 0\n",
    "\n",
    "                    attention_mask = torch.cat((attention_mask, extended_attention_mask), dim=1)\n",
    "                    position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n",
    "\n",
    "        outputs = self.language_model(\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        logits = outputs[0]\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            if attention_mask is not None:\n",
    "                shift_attention_mask = attention_mask[..., 1:]\n",
    "                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n",
    "                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n",
    "            else:\n",
    "                shift_logits = logits[..., :-1, :].contiguous()\n",
    "                shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n",
    "            )\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return LlavaCausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
