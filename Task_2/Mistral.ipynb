{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install trl accelerate bitsandbytes peft einops langchain wandb -qqq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments)\n",
    "from trl import SFTTrainer\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import dataloader, Dataset\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset as HFDataset\n",
    "from langchain.prompts import PromptTemplate\n",
    "import transformers\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "from docarray import BaseDoc, DocList\n",
    "from docarray.typing import NdArray\n",
    "from docarray.index import InMemoryExactNNIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    model_name = 'mistralai/Mistral-7B-Instruct-v0.2' ### Model Dependent\n",
    "    quantisation_4_bit = True\n",
    "    quantisation_8_bit = False\n",
    "    \n",
    "    batch_size = 1\n",
    "    grad_acc_steps = 8\n",
    "    device = 'cuda'\n",
    "    \n",
    "    # tokenizer.pad_token = tokenizer.eos_token # Mostly\n",
    "    # tokenizer.padding_side = 'right' # Model dependent again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ec032889e94eaa99f410856a7e05d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.model_name,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name,\n",
    "        # quantization_config=bnb_config if args.quantisation_4_bit else None,# 4-bit quantisation\n",
    "        # load_in_8bit = True if args.quantisation_8_bit else None, # 8-bit quantisation\n",
    "        device_map={\"\": 0}, # Single GPU\n",
    "        trust_remote_code=True\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:local_cache is not supported for \"file\" protocol\n",
      "WARNING:root:local_cache is not supported for \"file\" protocol\n"
     ]
    }
   ],
   "source": [
    "class Tweet(BaseDoc):\n",
    "    id: int\n",
    "    content: str\n",
    "    likes: int\n",
    "    date: str\n",
    "    username: str\n",
    "    media: str\n",
    "    content_embeds: NdArray\n",
    "    image_embeds: NdArray\n",
    "    inferred_company: str\n",
    "\n",
    "train_list = DocList[Tweet].pull('file://train_list')\n",
    "tweet_list = DocList[Tweet].pull('file://tweet_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc_index = InMemoryExactNNIndex[Tweet]()\n",
    "doc_index.index(tweet_list)\n",
    "\n",
    "train_index = InMemoryExactNNIndex[Tweet]()\n",
    "train_index.index(train_list)\n",
    "\n",
    "def get_rel_docs(query):\n",
    "    retrieved_docs, scores = doc_index.find(query, search_field='image_embeds', limit=3)\n",
    "    return retrieved_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_list.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helo\n",
      "hayii\n"
     ]
    }
   ],
   "source": [
    "x = \"\"\"helo\\nhayii\"\"\"\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataGen:\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\"\"\"<s>[INST] \n",
    "Using the given tweets as reference construct a tweet which has analogical similarity to those, conditioned on the fact that the post is a {views} post.\n",
    "\n",
    "Reference tweets: \n",
    "{ref}\n",
    "\n",
    "The tweet is written by the user {username} belong to {company}.\n",
    "\n",
    "New Tweet: {predict} </s>\"\"\")\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = HFDataset.from_pandas(data)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def classify_likes(self, likes):\n",
    "        if likes > 10000:\n",
    "            return \"Viral\"\n",
    "        elif likes > 1000:\n",
    "            return \"High Engagement\"\n",
    "        elif likes > 100:\n",
    "            return \"Moderate Engagement\"\n",
    "        else:\n",
    "            return \"Low Engagement\"\n",
    "\n",
    "    def get_ref_tweets(self, entry):\n",
    "        docs = get_rel_docs(np.array(entry['image_embeds']))\n",
    "        contents = docs.content\n",
    "        likes = docs.likes\n",
    "        users = docs.username\n",
    "        companies = docs.inferred_company\n",
    "\n",
    "        ref = \"\"\n",
    "\n",
    "        for content, like, user, company in zip(contents, likes, users, companies):\n",
    "            ref+=f\"\"\"Tweet: ```{content}```\\nUsername:{user}\\nCompany:{company}\\nLikes:{self.classify_likes(like)}\\n\\n\"\"\"\n",
    "            \n",
    "        return ref\n",
    "        \n",
    "    def generate_and_tokenize_prompt(self, entry):\n",
    "        \n",
    "        ref_tweets = self.get_ref_tweets(entry)\n",
    "        prompt = self.prompt.format(views=self.classify_likes(entry['likes']), ref=ref_tweets, username=entry['username'], company=entry['inferred_company'], predict=entry['content'])\n",
    "        return tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    def get_formatted_dataset(self):\n",
    "        return self.data.map(self.generate_and_tokenize_prompt)\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataGen(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<bound method DataGen.generate_and_tokenize_prompt of <__main__.DataGen object at 0x7fc3951f0250>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<bound method DataGen.generate_and_tokenize_prompt of <__main__.DataGen object at 0x7fc3951f0250>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cea678f6c174c13beeb68a83a74b2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = data.get_formatted_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "train_dataset = joblib.load(\"dataset.joblib\")\n",
    "# joblib.dump(dataset, \"dataset.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f15fdedf1443949bc9ebecd2bcce33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data = dict({\"prompt\":[\"\"\"\n",
    "#     <s>[INST] \n",
    "# Using the given tweets as reference construct a tweet which has analogical similarity to those, conditioned on the fact that the post is extremely viral.\n",
    "\n",
    "# Reference tweets: \n",
    "\n",
    "# Tweet: ```What a great day to BE part of the #BTSARMY. Shop the deluxe version of <mention>'s new album: <hyperlink> <hyperlink>```\n",
    "# Username: Target\n",
    "# Company: Target\n",
    "\n",
    "# Tweet: ```Your grand ideas will never go off track. ðŸ’¡ #GalaxyNote20 #GalaxyxBTS <mention> ðŸ‘” Learn more: <hyperlink> <hyperlink>```\n",
    "# Username: Samsung\n",
    "# Company: Samsung\n",
    "\n",
    "\n",
    "# The tweet is written by the user spotify belong to spotify.\n",
    "\n",
    "# New Tweet: ```Happy <mention> release day #ARMY! \n",
    "# #LoveYourselfAnswer is here ðŸ’œ \n",
    "# <hyperlink> <hyperlink>```\n",
    "# \"\"\" for _ in range(10)]})\n",
    "\n",
    "\n",
    "# max_length = 512\n",
    "\n",
    "# def transform(x):\n",
    "#     result = tokenizer(\n",
    "#         x[\"prompt\"],\n",
    "#         truncation=True,\n",
    "#         max_length=max_length,\n",
    "#         padding=\"max_length\",\n",
    "#     )\n",
    "#     result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "#     return result\n",
    "\n",
    "    \n",
    "# train_dataset=HFDataset.from_dict(data)\n",
    "# train_dataset = train_dataset.map(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'content', 'likes', 'date', 'username', 'media', 'content_embeds', 'image_embeds', 'inferred_company', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 9618\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da107b206f74e89bd25f069e18982cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_to_tensors(entry):\n",
    "\n",
    "    entry['input_ids'] = entry['input_ids'].squeeze()\n",
    "    entry['attention_mask'] =entry['attention_mask'].squeeze()\n",
    "    return entry\n",
    "\n",
    "train_dataset2=train_dataset.with_format(\"torch\")\n",
    "train_dataset2 = train_dataset2.map(convert_to_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([356])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset2['input_ids'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset = test_dataset,\n",
    "#     peft_config=peft_config,\n",
    "#     dataset_text_field=\"text\",\n",
    "#     max_seq_length=1024, # Adjust accordingly\n",
    "#     tokenizer=tokenizer,\n",
    "#     args=training_arguments,\n",
    "#     packing=True,\n",
    "# )\n",
    "\n",
    "# for name, module in trainer.model.named_modules():\n",
    "#     if \"norm\" in name:\n",
    "#         module = module.to(torch.float32)\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "# model.save_pretrained(\"output_dir\") # saves lora again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 85041152 || all params: 7326773248 || trainable%: 1.1606903765339511\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): MistralRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm()\n",
       "            (post_attention_layernorm): MistralRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm()\n",
       "      )\n",
       "      (lm_head): lora.Linear(\n",
       "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=32, out_features=32000, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "project = \"journal-finetune\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#         output_dir=output_dir,\n",
    "#         warmup_steps=1,\n",
    "#         per_device_train_batch_size=2,\n",
    "#         gradient_accumulation_steps=1,\n",
    "#         gradient_checkpointing=True,\n",
    "#         max_steps=500,\n",
    "#         learning_rate=2.5e-5, # Want a small lr for finetuning\n",
    "#         bf16=True,\n",
    "#         optim=\"paged_adamw_8bit\",\n",
    "#         logging_steps=25,              # When to start reporting loss\n",
    "#         logging_dir=\"./logs\",        # Directory for storing logs\n",
    "#         save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "#         save_steps=25,                # Save checkpoints every 50 steps\n",
    "#         evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "#         eval_steps=25,               # Evaluate and save checkpoints every 50 steps\n",
    "#         do_eval=True,                # Perform evaluation at the end of training\n",
    "#         report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "#         run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "#     )\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results_latest\",\n",
    "    per_device_train_batch_size=args.batch_size,\n",
    "    gradient_accumulation_steps=args.grad_acc_steps,\n",
    "    optim='paged_adamw_32bit',\n",
    "    # save_steps=250,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "    save_steps=25, \n",
    "    learning_rate=2e-4,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to = 'wandb'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([356])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset2['input_ids'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset2,\n",
    "    # eval_dataset=tokenized_val_dataset,\n",
    "    args=training_arguments,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    <s>[INST] \n",
    "Using the given tweets as reference construct a tweet which has analogical similarity to those, conditioned on the fact that the post is extremely viral.\n",
    "\n",
    "Reference tweets: \n",
    "\n",
    "Tweet: ```What a great day to BE part of the #BTSARMY. Shop the deluxe version of <mention>'s new album: <hyperlink> <hyperlink>```\n",
    "Username: Target\n",
    "Company: Target\n",
    "\n",
    "Tweet: ```Your grand ideas will never go off track. ðŸ’¡ #GalaxyNote20 #GalaxyxBTS <mention> ðŸ‘” Learn more: <hyperlink> <hyperlink>```\n",
    "Username: Samsung\n",
    "Company: Samsung\n",
    "\n",
    "\n",
    "The tweet is written by the user spotify belong to spotify.\n",
    "\n",
    "New Tweet:\"\"\"\n",
    "\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    tokens = model.generate(**model_input, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \n",
      "    <s> [INST] \n",
      "Using the given tweets as reference construct a tweet which has analogical similarity to those, conditioned on the fact that the post is extremely viral.\n",
      "\n",
      "Reference tweets: \n",
      "\n",
      "Tweet: ```What a great day to BE part of the #BTSARMY. Shop the deluxe version of <mention>'s new album: <hyperlink> <hyperlink>```\n",
      "Username: Target\n",
      "Company: Target\n",
      "\n",
      "Tweet: ```Your grand ideas will never go off track. ðŸ’¡ #GalaxyNote20 #GalaxyxBTS <mention> ðŸ‘” Learn more: <hyperlink> <hyperlink>```\n",
      "Username: Samsung\n",
      "Company: Samsung\n",
      "\n",
      "\n",
      "The tweet is written by the user spotify belong to spotify.\n",
      "\n",
      "New Tweet:</s>\n",
      "```\n",
      "ðŸ’¡ðŸ’¡ðŸ’¡\n",
      "\n",
      "<hyperlink> <hyperlink>\n",
      "\n",
      "<hyperlink> <hyperlink>\n",
      "\n",
      "<hyperlink> <hyperlink>\n",
      "\n",
      "<hyperlink> <hyperlink>\n",
      "\n",
      "<hyperlink> <hyperlink>\n",
      "\n",
      "<hyperlink> <hyperlink>\n",
      "\n",
      "<hyperlink> <hyperlink>\n",
      "\n",
      "<hyperlink> <hyperlink>\n",
      "\n",
      "<hyper\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4115773,
     "sourceId": 7133242,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4144553,
     "sourceId": 7172929,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
