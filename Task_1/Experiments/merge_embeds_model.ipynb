{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30117,"status":"ok","timestamp":1701429462698,"user":{"displayName":"team_31","userId":"01552236247297192941"},"user_tz":-330},"id":"ZeFf_ro-lnV3","outputId":"31e712d6-f810-40d0-8c73-e91badad83a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sH9aRCo4_p_e"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import mean_squared_error\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MmeRJ2k4u933"},"outputs":[],"source":["merged_embeddings = np.load('/content/drive/MyDrive/merged_embeddings.npy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GyNJ3U2H5wk0"},"outputs":[],"source":["likes = np.load('/content/drive/MyDrive/likes.npy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8HOqeUCEvTC-"},"outputs":[],"source":["merged_embeddings = merged_embeddings.reshape(51440,1,1792)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Y2Avic24KXU"},"outputs":[],"source":["train_embeddings = merged_embeddings[0:41152]\n","val_embeddings = merged_embeddings[41152:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZPxtEtf6Ec6"},"outputs":[],"source":["train_likes = likes[0:41152]\n","val_likes = likes[41152:]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1701429478551,"user":{"displayName":"team_31","userId":"01552236247297192941"},"user_tz":-330},"id":"FAI9Z3fA42m6","outputId":"59a5e4d3-a5b0-4b6e-a714-de4d40b90490"},"outputs":[{"data":{"text/plain":["array([   1, 2750,   57, ...,  307,  806,   16])"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["train_likes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1701429478551,"user":{"displayName":"team_31","userId":"01552236247297192941"},"user_tz":-330},"id":"Zgh-6_zi4spl","outputId":"a9a3a202-75b1-4a7a-a9a3-b578e6feabc2"},"outputs":[{"data":{"text/plain":["array([974,   0, 300, ..., 707,   8, 908])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["val_likes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4olzijwO6051"},"outputs":[],"source":["train_labels = train_likes\n","val_labels = val_likes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1701429478551,"user":{"displayName":"team_31","userId":"01552236247297192941"},"user_tz":-330},"id":"oJwZN6C2JsBE","outputId":"7f1590f0-5107-40b1-99d7-64bc87373fcd"},"outputs":[{"data":{"text/plain":["((41152, 1, 1792), (10288, 1, 1792))"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["train_embeddings.shape, val_embeddings.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1701429478551,"user":{"displayName":"team_31","userId":"01552236247297192941"},"user_tz":-330},"id":"ur0y2rA0ORBG","outputId":"030f5bb8-c6f7-46b9-a922-791b886b06f2"},"outputs":[{"data":{"text/plain":["array([[[1],\n","        [2],\n","        [3]],\n","\n","       [[4],\n","        [5],\n","        [6]]])"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["np.expand_dims(np.array([[1, 2, 3], [4, 5, 6]]), axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aSQsUf4EKoak"},"outputs":[],"source":["model  = tf.keras.Sequential([\n","    # tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(1792,1)),\n","    # tf.keras.layers.GlobalMaxPooling1D(),\n","    tf.keras.layers.LSTM(128),\n","    tf.keras.layers.Dense(64, activation=\"relu\"),\n","    tf.keras.layers.Dense(32, activation=\"relu\"),\n","    tf.keras.layers.Dense(1)\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ah2fVh6bLz_V"},"outputs":[],"source":["model.compile(optimizer='adam', loss='mse', metrics=tf.keras.metrics.RootMeanSquaredError())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hsuvg4gpMoX1"},"outputs":[],"source":["train_embeddings = tf.constant(np.expand_dims(np.squeeze(train_embeddings, axis=1), axis=-1))\n","val_embeddings = tf.constant(np.expand_dims(np.squeeze(val_embeddings, axis=1), axis=-1))\n","train_labels = tf.constant(train_labels)\n","val_labels = tf.constant(val_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1701429641753,"user":{"displayName":"team_31","userId":"01552236247297192941"},"user_tz":-330},"id":"54hzxa2sA4a4","outputId":"c508f9ac-fc07-4394-e337-2fd09f5459a3"},"outputs":[{"data":{"text/plain":["(TensorShape([41152, 1792, 1]), TensorShape([10288, 1792, 1]))"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["train_embeddings.shape, val_embeddings.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60706,"status":"ok","timestamp":1701429719453,"user":{"displayName":"team_31","userId":"01552236247297192941"},"user_tz":-330},"id":"jqM1gganMhA2","outputId":"04501414-5f1f-417b-a621-19c1c535902c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/3\n","81/81 [==============================] - 23s 236ms/step - loss: 24780052.0000 - root_mean_squared_error: 4977.9565 - val_loss: 14342334.0000 - val_root_mean_squared_error: 3787.1274\n","Epoch 2/3\n","81/81 [==============================] - 19s 230ms/step - loss: 24366782.0000 - root_mean_squared_error: 4936.2720 - val_loss: 14086069.0000 - val_root_mean_squared_error: 3753.1411\n","Epoch 3/3\n","81/81 [==============================] - 18s 229ms/step - loss: 24292898.0000 - root_mean_squared_error: 4928.7827 - val_loss: 14085980.0000 - val_root_mean_squared_error: 3753.1294\n"]}],"source":["history = model.fit(train_embeddings, train_labels, validation_data=(val_embeddings, val_labels), epochs=3, batch_size=512, validation_batch_size=256)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":366},"executionInfo":{"elapsed":2072,"status":"error","timestamp":1701427697702,"user":{"displayName":"team_31","userId":"01552236247297192941"},"user_tz":-330},"id":"2qDg3NLjgoPJ","outputId":"450d2551-ae8e-4cb4-abea-49f68903cb1e"},"outputs":[],"source":["# Create a custom dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, embeddings, labels):\n","        self.embeddings = embeddings\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        return torch.tensor(self.embeddings[idx]), torch.tensor(self.labels[idx])\n","\n","# Define your regression model\n","class ShallowNetwork(nn.Module):\n","    def __init__(self, input_size, num_filters, kernel_size):\n","        super(ShallowNetwork, self).__init__()\n","        self.conv1d = nn.Conv1d(in_channels=input_size, out_channels=num_filters,\n","                                kernel_size=kernel_size)\n","        self.relu = nn.ReLU()\n","        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n","        self.fc1 = nn.Linear(64, 32)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(32, 1)\n","\n","\n","    def forward(self, x):\n","        x = self.conv1d(x)\n","        x = self.relu(x)\n","\n","        # Global Max Pooling\n","        x = self.global_pooling(x)\n","        x = x.view(x.size(0), -1)  # Flatten the output\n","\n","        # Fully connected layers\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","\n","\n","# Check for GPU availability\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize model, criterion, optimizer on GPU\n","model = ShallowNetwork(input_size=512, kernel_size=3, num_filters=64).to(device)\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Prepare your data (embeddings and labels) for training and validation on GPU\n","# Replace embeddings_train, labels_train, embeddings_val, labels_val with your data\n","embeddings_train = train_embeddings\n","labels_train = train_labels\n","embeddings_val = val_embeddings\n","labels_val = val_labels\n","\n","# Move data to GPU\n","embeddings_train = torch.tensor(embeddings_train).to(device)\n","labels_train = torch.tensor(labels_train).to(device)\n","embeddings_val = torch.tensor(embeddings_val).to(device)\n","labels_val = torch.tensor(labels_val).to(device)\n","\n","# Create DataLoader for training and validation\n","train_dataset = CustomDataset(embeddings_train, labels_train)\n","train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_dataset = CustomDataset(embeddings_val, labels_val)\n","val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n","\n","# Training and validation loop with RMSE and loss calculation on GPU\n","epochs = 10\n","for epoch in range(epochs):\n","    # Training loop\n","    model.train()\n","    total_loss_train = 0.0\n","    for inputs, targets in train_dataloader:\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs.float())  # Assuming inputs are float type\n","        loss = criterion(outputs, targets.unsqueeze(1).float())  # Calculate loss\n","        loss.backward()  # Backpropagation\n","        optimizer.step()\n","        total_loss_train += loss.item() * inputs.size(0)  # Accumulate the total loss\n","\n","    epoch_loss_train = total_loss_train / len(train_dataloader.dataset)\n","    rmse_train = torch.sqrt(torch.tensor(epoch_loss_train, device=device))  # Calculate RMSE for training\n","\n","    # Validation loop\n","    model.eval()\n","    total_loss_val = 0.0\n","    with torch.no_grad():\n","        for inputs, targets in val_dataloader:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = model(inputs.float())  # Assuming inputs are float type\n","            loss = criterion(outputs, targets.unsqueeze(1).float())  # Calculate loss\n","            total_loss_val += loss.item() * inputs.size(0)  # Accumulate the total loss\n","\n","    epoch_loss_val = total_loss_val / len(val_dataloader.dataset)\n","    rmse_val = torch.sqrt(torch.tensor(epoch_loss_val, device=device))  # Calculate RMSE for validation\n","\n","    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {epoch_loss_train:.4f}, Train RMSE: {rmse_train:.4f}, \\\n","          Val Loss: {epoch_loss_val:.4f}, Val RMSE: {rmse_val:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1701413126828,"user":{"displayName":"Harshith M R","userId":"08081188079244524473"},"user_tz":-330},"id":"LI0pxoKlHFlJ","outputId":"83e1b4c9-55ae-45bc-b167-2551e6f903ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([64, 64, 508])\n"]}],"source":["# Tetsing\n","k = nn.Conv1d(1,64,kernel_size=5)\n","input = torch.randn(64, 1, 512)\n","print(k(input).shape) # -> torch.Size([1, 64, 736])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":418,"status":"ok","timestamp":1701413040059,"user":{"displayName":"Harshith M R","userId":"08081188079244524473"},"user_tz":-330},"id":"IvqngFAIHpyZ","outputId":"f526994b-47e0-4817-965e-77b4be77a2f1"},"outputs":[{"data":{"text/plain":["tensor([[[-1.0701e+00, -4.1138e-02, -3.8949e-02,  3.7355e-01,  4.2960e-01,\n","          -7.8454e-01, -3.8276e-01,  9.5227e-01, -5.5946e-01,  1.3074e+00,\n","          -9.0360e-01,  5.1104e-01,  1.0577e+00, -7.0164e-01, -3.7643e-01,\n","          -1.0538e+00, -3.7156e-01, -3.7437e-01, -1.1957e+00, -8.4552e-01,\n","          -9.7767e-01,  6.1196e-02,  2.8820e+00,  3.3508e-01, -1.1501e+00,\n","           1.2041e+00, -8.6040e-01,  1.3077e+00, -1.1140e+00,  2.0064e-01,\n","           1.1927e+00, -8.3233e-01,  8.4071e-01, -6.2430e-02,  7.7285e-01,\n","           1.4545e+00,  4.8790e-01, -5.1689e-01, -3.5740e-01,  1.7161e+00,\n","           1.3773e+00,  9.0333e-01, -1.7872e-01,  4.5178e-01, -8.1685e-01,\n","          -1.6507e-01,  4.6807e-01, -9.1544e-01, -7.8223e-01, -6.7237e-01,\n","          -3.2841e+00, -6.9346e-01,  2.3468e-01,  1.7971e-01,  6.0856e-01,\n","           2.2847e-01,  9.7044e-01, -1.2593e-01, -5.6233e-01, -8.6351e-01,\n","          -7.6913e-01, -1.2517e+00, -1.1104e+00, -1.5638e+00,  1.4023e+00,\n","          -6.4234e-01,  7.6550e-01, -1.0494e+00,  1.8155e+00,  1.6760e-01,\n","          -9.3669e-01, -9.1294e-01,  1.5387e+00, -4.6222e-01,  1.2451e+00,\n","           5.5371e-01, -8.2372e-01, -1.6217e+00,  6.6238e-01, -8.3391e-01,\n","          -5.4453e-03,  9.1581e-01, -4.2009e-01, -4.6224e-01,  3.9984e-01,\n","           3.7382e-01,  4.6760e-01, -4.7286e-01, -2.1731e+00, -7.9361e-01,\n","          -1.7240e+00, -8.4796e-01,  1.8873e+00,  1.4584e-01, -1.1334e-01,\n","           1.7771e+00,  8.2859e-01, -2.8837e+00, -2.2070e+00, -1.7016e+00,\n","           2.8277e-02,  1.1065e-01, -8.5899e-01, -2.2893e-01,  9.4387e-02,\n","           3.5010e-01,  2.4195e-01,  1.0391e+00,  7.1365e-01, -7.4650e-01,\n","          -5.6395e-01,  5.1321e-01, -6.2427e-01, -1.6651e+00,  2.0679e-01,\n","          -9.1743e-01, -8.8886e-01, -1.6638e-01,  3.0948e-01, -1.3069e+00,\n","           1.2954e-01, -2.7332e-01, -1.5737e+00, -1.2226e+00,  8.5434e-02,\n","          -1.7572e+00,  3.2567e-01, -1.7918e+00,  2.7540e-01, -5.8499e-01,\n","           3.4675e-01,  7.7796e-01,  1.0183e-01, -9.8656e-01, -5.1877e-01,\n","          -4.4425e-01, -2.2889e-01, -7.4545e-01, -2.0956e-01,  1.8353e-02,\n","          -1.6638e+00,  1.4681e-01,  1.3030e+00,  3.3821e+00, -1.0608e+00,\n","           1.1929e-01, -1.4517e+00,  2.3125e+00, -1.9707e-01, -1.1805e+00,\n","          -1.1039e+00, -1.0098e+00,  9.2898e-01, -4.9873e-01, -1.8631e-01,\n","          -1.0030e+00, -4.9022e-01, -1.9036e+00,  4.2608e-01, -3.8603e-01,\n","          -8.1819e-01,  7.9244e-01, -5.2761e-01, -6.5082e-01,  1.3686e-01,\n","          -2.6584e-02, -9.3776e-01, -1.6003e+00, -1.0257e-01,  4.6107e-01,\n","          -8.6867e-01,  1.9423e+00,  6.7964e-01, -2.1159e+00,  1.8511e-01,\n","           5.0894e-01,  9.1090e-01,  1.5409e+00, -9.7563e-01, -8.5006e-01,\n","          -2.0796e-01, -5.5850e-01,  4.9621e-01,  1.6639e-01,  8.4207e-01,\n","          -1.2146e+00, -6.2123e-01, -1.9357e+00, -1.4452e+00, -3.1600e-01,\n","          -1.3624e-01, -2.4861e-01,  1.7109e+00, -1.7093e-01, -8.4566e-02,\n","          -1.8068e+00, -1.2792e+00, -9.9902e-01, -1.6661e+00, -1.7868e+00,\n","           2.1476e-01, -1.7383e+00, -5.1639e-01, -2.6235e-01,  1.4293e+00,\n","          -1.5603e-01, -1.1100e+00, -8.5095e-01, -8.4825e-01,  4.1790e-02,\n","           1.1865e+00, -1.6865e-01, -1.1854e+00,  1.4465e+00,  9.2612e-01,\n","          -2.7002e-01, -1.6220e+00, -2.7380e-01,  4.6739e-01, -1.2062e+00,\n","          -1.1881e+00, -4.5711e-01,  8.1032e-01,  8.7997e-01,  7.0511e-01,\n","           2.0206e+00, -1.9337e-01,  7.5367e-01, -1.0163e+00, -1.9852e-01,\n","           2.3187e-02,  9.6494e-01, -2.2581e+00,  4.3544e-01,  1.0756e+00,\n","           1.9620e-02,  3.6080e-01,  3.0045e-01,  6.2582e-01, -5.5535e-01,\n","           1.1542e+00, -1.3046e+00, -8.8710e-01,  6.3406e-01,  3.0524e-01,\n","          -2.7883e-01, -3.4695e-01, -1.0226e+00, -5.3438e-02,  5.8233e-01,\n","          -1.3924e+00, -1.1480e+00, -7.9530e-01,  2.1236e+00, -7.0866e-02,\n","           4.9451e-02,  1.6673e+00,  6.5455e-01, -8.6158e-01,  6.6018e-01,\n","           9.5527e-01, -1.3993e+00, -9.5684e-02, -4.0683e-01, -7.4773e-01,\n","           6.6905e-01, -2.7900e-02,  1.5674e-01,  8.0717e-01, -8.2635e-01,\n","           2.2438e-02, -1.4121e+00,  1.5240e+00,  6.5659e-01, -1.0028e-01,\n","          -5.6846e-01, -8.2920e-01, -4.5859e-02,  1.3116e+00, -9.5124e-01,\n","           1.2838e+00,  9.6084e-01,  9.9523e-01, -1.2384e+00,  7.2508e-01,\n","           8.2478e-01, -1.7678e+00, -4.9363e-01,  3.2474e-01, -3.4818e-01,\n","           3.8670e-01, -3.1078e-02,  5.5402e-01,  7.0460e-01,  8.7318e-01,\n","           9.8134e-01,  7.2552e-02, -1.2693e-01,  5.7644e-01, -1.4191e+00,\n","           3.3285e-01, -1.2226e+00,  6.9436e-01,  1.7843e+00, -4.2666e-01,\n","          -3.0997e-01,  1.1799e+00,  2.2522e-02, -1.3928e+00, -5.9799e-01,\n","           2.5761e-01, -1.0877e-01, -3.1105e-01,  1.6552e+00,  3.7092e-01,\n","           6.7516e-02, -2.7777e-01,  1.4499e-01, -1.9758e+00, -1.1543e+00,\n","           7.9180e-01, -9.9808e-01,  4.3856e-01,  9.5997e-01, -1.0017e+00,\n","           6.1300e-01, -1.0879e+00,  8.3193e-01,  1.5227e+00,  1.1115e-02,\n","           3.8683e-01,  9.9997e-01, -4.6047e-01,  2.5801e-01,  2.8967e-01,\n","          -1.6115e+00, -1.0785e-01,  4.8763e-01,  7.1042e-02, -2.2490e+00,\n","           8.6093e-01, -2.6189e-01,  4.8396e-01, -1.9632e+00, -4.6186e-01,\n","          -4.0936e-01,  1.1439e+00, -3.0339e-01, -1.1060e+00,  5.7169e-01,\n","           1.2777e-01, -9.1824e-01, -8.0054e-01, -1.4871e-03, -2.8858e-01,\n","           1.2739e+00,  2.2999e-01, -1.2394e+00,  6.1722e-01, -9.2920e-01,\n","          -1.2527e+00,  1.3066e+00,  1.4773e-01, -8.3919e-02, -5.9161e-01,\n","          -3.4851e-01,  1.5084e-01, -1.2058e+00, -2.1748e+00,  1.2339e+00,\n","           1.9294e+00,  1.7142e+00,  9.3807e-01,  1.4952e+00, -1.1771e+00,\n","           4.5918e-01,  9.5107e-01, -1.0917e+00,  1.6040e+00,  1.8599e+00,\n","          -1.8351e-01, -9.1017e-01, -1.8964e+00,  9.7632e-01,  7.1929e-01,\n","          -4.4804e-01, -4.9571e-02, -1.1993e+00, -8.4304e-01, -1.8253e+00,\n","          -3.7684e-01,  8.7119e-01,  5.0629e-01,  1.2024e+00,  1.4338e+00,\n","          -2.6463e-01,  1.2078e+00,  1.3848e+00, -1.3733e+00,  3.2784e-01,\n","          -5.4180e-01, -5.0749e-01,  4.2172e-01,  2.2925e-02,  1.6795e+00,\n","           1.5855e+00,  6.5309e-01, -3.0827e-01, -3.9749e-01,  9.8699e-01,\n","           8.6407e-01, -1.4781e+00, -2.0725e+00,  2.3005e-01, -5.5182e-01,\n","          -2.5439e-01,  6.1943e-01, -1.1068e+00,  1.0039e+00, -3.0685e-02,\n","           4.4346e-02,  1.3509e+00,  9.2293e-01, -2.2578e+00, -5.6885e-01,\n","           7.7263e-01, -1.1650e+00,  2.9318e-01, -9.5701e-01, -2.0314e+00,\n","          -5.9131e-01, -1.6464e+00,  6.2415e-01, -1.3508e+00, -2.1687e-01,\n","          -1.3087e+00,  6.1204e-01,  4.0789e-02,  1.4803e+00, -5.8195e-01,\n","           1.8321e-01, -1.8656e+00,  1.7405e-01, -4.2682e-01,  1.0011e+00,\n","          -6.1487e-02, -1.1992e+00,  9.9702e-02, -1.0536e+00,  2.1414e-01,\n","           7.1581e-02,  2.0127e-01,  1.7972e+00,  1.6220e+00,  3.0995e-01,\n","          -5.6767e-01, -4.0806e-01, -7.8150e-02,  1.6681e+00, -9.8458e-01,\n","           3.1257e-01, -2.8526e-02, -5.1172e-02,  1.2376e+00,  3.0875e-01,\n","          -4.0763e-01,  4.5484e-01,  1.6685e+00, -1.0901e+00, -9.9281e-01,\n","           1.5935e+00,  3.3294e-01, -1.6886e-01, -6.9131e-01, -7.0970e-01,\n","           5.4263e-01,  8.7767e-01,  3.2698e-01, -7.8661e-02, -1.0958e-01,\n","           9.8687e-01,  7.4372e-01, -2.0806e+00, -3.2916e-01, -3.2971e-01,\n","          -1.0105e+00,  7.6697e-01, -9.5931e-01,  6.1407e-01, -3.3221e-01,\n","          -6.2447e-01,  2.7699e+00, -9.5370e-01,  9.0755e-01,  7.8777e-01,\n","           3.5318e-01, -4.7123e-01, -1.4826e+00, -1.9179e+00, -4.3676e-01,\n","           8.4140e-01, -6.0824e-01, -1.2428e+00, -9.3591e-01, -2.7277e-01,\n","           1.5886e+00,  1.5100e+00,  6.0501e-01,  9.7469e-02,  1.7004e+00,\n","           6.8916e-01,  1.3817e+00, -1.3142e+00,  4.5370e-01, -4.4550e-01,\n","           4.1767e-01,  9.1449e-01,  8.6019e-03,  2.0091e+00, -1.0673e+00,\n","           5.9056e-01, -9.4807e-01,  9.0060e-01, -7.1702e-02, -1.4438e-01,\n","           1.7578e-01, -1.1329e+00, -8.4483e-01,  3.8982e-01,  1.1558e+00,\n","          -1.5269e+00,  7.8494e-01, -2.6732e-01, -2.0533e+00,  2.2766e-02,\n","          -2.9405e-02,  1.1476e+00,  1.4666e-01,  1.0657e-01,  8.1630e-01,\n","           1.8777e-01,  1.6851e+00, -2.1714e-01,  2.3035e-01,  1.2431e+00,\n","           7.8264e-02, -1.2867e+00,  6.0517e-01, -4.4219e-01,  1.2715e-01,\n","           5.2158e-03, -6.7721e-01,  7.4940e-01, -8.3865e-01, -2.5282e-01,\n","          -8.0323e-01,  4.6217e-02,  1.2302e+00,  3.0340e-01,  2.5126e-01,\n","           1.3749e+00, -1.0969e+00, -1.3709e-02, -1.8434e+00, -8.2596e-01,\n","          -5.5412e-01, -1.0731e+00,  2.5619e-01,  4.0477e-01,  9.7558e-01,\n","          -2.3643e+00,  2.4652e-01, -5.9684e-01,  5.6293e-01,  8.0874e-01,\n","           4.2563e-01, -8.0927e-01,  1.5568e+00, -6.8612e-01, -1.3230e-01,\n","           3.7977e-01,  2.0674e-01, -5.8670e-01,  1.3269e+00, -7.8875e-02,\n","           8.4296e-01,  5.1422e-01, -1.5445e+00,  1.8002e+00, -8.2005e-01,\n","           6.9779e-01,  8.1666e-01,  3.3525e-01,  1.4919e-01,  1.1122e+00,\n","           4.6517e-01,  1.4308e-01,  6.5705e-01, -5.0081e-01, -1.9280e-01,\n","          -2.1483e-01, -4.3719e-01,  1.1963e+00, -1.0937e+00,  7.9335e-02,\n","          -8.9883e-01,  5.0921e-01, -2.2259e-01,  4.7808e-01,  1.6456e+00,\n","          -1.8432e+00,  1.4948e+00, -1.3816e+00, -5.3056e-01, -8.0239e-01,\n","           1.4689e+00, -5.4253e-01, -9.8042e-01, -3.2123e-01,  1.5022e+00,\n","           1.2496e+00,  1.2566e+00,  1.6710e+00, -4.8913e-01, -2.2578e+00,\n","           8.9821e-01, -4.7007e-01,  1.3506e+00, -4.4583e-01, -1.1399e-01,\n","          -1.1586e+00,  6.8254e-01,  4.0665e-02,  2.8646e-01,  1.3783e+00,\n","           1.0686e-02,  1.1112e+00, -1.8043e+00, -4.0952e-01, -1.4010e+00,\n","           3.5185e-01,  7.9642e-01,  8.0495e-01, -7.4689e-01, -2.2048e+00,\n","           1.4431e-02,  6.4031e-01,  6.5850e-01,  1.1314e-02, -5.1340e-01,\n","           2.0944e-01,  9.5285e-01, -6.8416e-01, -9.1861e-01, -3.0672e-01,\n","          -5.9929e-01,  2.1936e+00, -1.1322e+00, -2.4606e+00,  1.1220e+00,\n","          -7.8613e-01,  8.5375e-01, -2.7838e-01,  3.4951e-01, -9.6008e-01,\n","           9.7554e-01,  3.0885e-01,  4.5458e-01,  6.3478e-01,  5.3555e-01,\n","           3.0633e-01,  6.9540e-01, -7.4002e-01,  7.5051e-01,  6.0566e-01,\n","          -1.8304e-01, -1.6237e+00, -2.2588e-01, -5.2264e-01, -5.0910e-01,\n","           2.0929e-01, -1.3607e+00, -7.6593e-01, -7.5025e-03,  4.1840e-01,\n","           1.2904e+00,  1.9327e+00, -1.3466e+00,  5.7771e-01,  1.1994e+00,\n","           2.9669e-01, -2.6667e-01,  3.9346e-01, -3.1833e-01,  3.3366e-01,\n","           5.3145e-01,  1.1597e+00,  9.3546e-02, -2.1427e-01, -1.1556e+00,\n","           7.0636e-01, -7.6167e-01,  4.9561e-01, -8.5166e-01, -1.0311e-01,\n","          -1.0853e+00,  1.9345e+00, -1.1986e-01, -1.0108e+00, -5.9745e-01,\n","          -1.6024e+00,  5.5293e-01, -5.0081e-01,  1.7123e+00,  1.4533e+00,\n","           7.4883e-01, -6.0929e-01,  2.9960e-01,  6.6640e-01, -9.7800e-01,\n","          -6.8703e-02,  3.2669e-01, -1.0171e+00,  1.8949e+00, -1.2550e+00,\n","           8.7641e-01,  1.6153e+00, -1.2653e+00, -1.5563e-01,  5.3238e-02,\n","           9.0289e-01,  9.8689e-01, -5.9106e-01,  1.5037e+00, -1.3108e+00,\n","          -2.7844e-01,  9.0617e-01, -5.9476e-01,  5.3803e-01,  5.4310e-01]]])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["input"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":511,"status":"ok","timestamp":1701413760341,"user":{"displayName":"Harshith M R","userId":"08081188079244524473"},"user_tz":-330},"id":"-0rpqRQdKXeH","outputId":"8caeb67c-a196-46aa-bcf0-29e951d39cb3"},"outputs":[{"data":{"text/plain":["torch.Size([208801, 1, 512])"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["embeddings_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1701413072524,"user":{"displayName":"Harshith M R","userId":"08081188079244524473"},"user_tz":-330},"id":"P267wzZJHt9-","outputId":"06d5d8a9-717f-4155-ea0e-2c23b4dd3b2b"},"outputs":[{"data":{"text/plain":["tensor([[[ 0.1971,  0.5489,  0.6214,  ..., -0.0104,  0.7965, -0.4791],\n","         [ 0.0739, -0.0224,  0.6053,  ..., -0.2242,  0.3812,  0.3371],\n","         [-0.4516, -0.3743, -0.0386,  ..., -0.6171,  0.2513, -0.9311],\n","         ...,\n","         [ 0.0981, -0.4668, -0.2003,  ..., -0.9016,  0.0403,  0.1120],\n","         [-0.0456, -0.0982,  0.5883,  ..., -0.0868,  0.2144,  0.3239],\n","         [ 0.6192, -0.0426,  0.3622,  ...,  0.1039,  0.8762,  0.2422]]],\n","       grad_fn=<ConvolutionBackward0>)"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["k(input)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
